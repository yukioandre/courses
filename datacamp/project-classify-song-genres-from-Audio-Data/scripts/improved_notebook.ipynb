{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONTEXTUALIZANDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"#### Settings ###########\\nimport seaborn as sns\\n\\nsns.set()\\nimport pandas as pd\\n\\npd.options.display.max_columns = 500\\npd.options.display.max_rows = 500\\nimport matplotlib.pyplot as plt\\n\\nplt.style.use(\\\"classic\\\")\\nplt.rcParams[\\\"figure.figsize\\\"] = [10, 5]\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"#### Settings ###########\\nimport seaborn as sns\\n\\nsns.set()\\nimport pandas as pd\\n\\npd.options.display.max_columns = 500\\npd.options.display.max_rows = 500\\nimport matplotlib.pyplot as plt\\n\\nplt.style.use(\\\"classic\\\")\\nplt.rcParams[\\\"figure.figsize\\\"] = [10, 5]\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Settings ###########\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.max_columns = 500\n",
    "pd.options.display.max_rows = 500\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"classic\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 5]\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há um projeto no Datacamp chamado <b>Classify Song Genres from Audio Data</b>. Nele, construímos um modelo de árvore de decisão, com redução de dimensionalidade via PCA, para classificar estilos musicais. O projeto possui uma falha de Data Leakage e acredito que a estrutura possa ser melhorada com um Pipeline. Este notebook fará essas alterações para explorar um código melhor estruturado e um modelo com melhor acurácia.\n",
    "\n",
    "Primeiro, o projeto rodou uma árvore de decisão e obteve acurácia de 86%. Em seguida, testou-se uma regressão logística, que atingiu 89%. Abaixo, o classification report dos dois modelos testados:\n",
    "<img src='modelo_desbalanceado.png'>\n",
    "\n",
    "Em seguida, construiu-se modelos com os dados balanceados. Nesse caso, a acurácia foi de 77% e 83%. O classification report:\n",
    "<img src='modelo_balanceado.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. PRINCIPAL: MODELO UTILIZANDO PIPELINE E RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>genre_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.416675</td>\n",
       "      <td>0.675894</td>\n",
       "      <td>0.634476</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.177647</td>\n",
       "      <td>0.159310</td>\n",
       "      <td>165.922</td>\n",
       "      <td>0.576661</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.374408</td>\n",
       "      <td>0.528643</td>\n",
       "      <td>0.817461</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.105880</td>\n",
       "      <td>0.461818</td>\n",
       "      <td>126.957</td>\n",
       "      <td>0.269240</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.745566</td>\n",
       "      <td>0.701470</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>0.124595</td>\n",
       "      <td>100.260</td>\n",
       "      <td>0.621661</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134</td>\n",
       "      <td>0.452217</td>\n",
       "      <td>0.513238</td>\n",
       "      <td>0.560410</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>0.096567</td>\n",
       "      <td>0.525519</td>\n",
       "      <td>114.290</td>\n",
       "      <td>0.894072</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>0.988306</td>\n",
       "      <td>0.255661</td>\n",
       "      <td>0.979774</td>\n",
       "      <td>0.973006</td>\n",
       "      <td>0.121342</td>\n",
       "      <td>0.051740</td>\n",
       "      <td>90.241</td>\n",
       "      <td>0.034018</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   track_id  acousticness  danceability    energy  instrumentalness  liveness  \\\n",
       "0         2      0.416675      0.675894  0.634476          0.010628  0.177647   \n",
       "1         3      0.374408      0.528643  0.817461          0.001851  0.105880   \n",
       "2         5      0.043567      0.745566  0.701470          0.000697  0.373143   \n",
       "3       134      0.452217      0.513238  0.560410          0.019443  0.096567   \n",
       "4       153      0.988306      0.255661  0.979774          0.973006  0.121342   \n",
       "\n",
       "   speechiness    tempo   valence genre_top  \n",
       "0     0.159310  165.922  0.576661   Hip-Hop  \n",
       "1     0.461818  126.957  0.269240   Hip-Hop  \n",
       "2     0.124595  100.260  0.621661   Hip-Hop  \n",
       "3     0.525519  114.290  0.894072   Hip-Hop  \n",
       "4     0.051740   90.241  0.034018      Rock  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Import libraries\\nimport pandas as pd\\n\\n# Read in track metadata with genre labels\\ntracks = pd.read_csv(\\\"fma-rock-vs-hiphop.csv\\\")\\n\\n# Read in track metrics with the features\\nechonest_metrics = pd.read_json(\\\"echonest-metrics.json\\\", precise_float=True)\\n\\n# Merge the relevant columns of tracks and echonest_metrics\\necho_tracks = pd.merge(\\n    echonest_metrics, tracks[[\\\"track_id\\\", \\\"genre_top\\\"]], on=\\\"track_id\\\"\\n)\\n\\n# Inspect the resultant dataframe\\necho_tracks.head()\";\n",
       "                var nbb_formatted_code = \"# Import libraries\\nimport pandas as pd\\n\\n# Read in track metadata with genre labels\\ntracks = pd.read_csv(\\\"fma-rock-vs-hiphop.csv\\\")\\n\\n# Read in track metrics with the features\\nechonest_metrics = pd.read_json(\\\"echonest-metrics.json\\\", precise_float=True)\\n\\n# Merge the relevant columns of tracks and echonest_metrics\\necho_tracks = pd.merge(\\n    echonest_metrics, tracks[[\\\"track_id\\\", \\\"genre_top\\\"]], on=\\\"track_id\\\"\\n)\\n\\n# Inspect the resultant dataframe\\necho_tracks.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Read in track metadata with genre labels\n",
    "tracks = pd.read_csv(\"fma-rock-vs-hiphop.csv\")\n",
    "\n",
    "# Read in track metrics with the features\n",
    "echonest_metrics = pd.read_json(\"echonest-metrics.json\", precise_float=True)\n",
    "\n",
    "# Merge the relevant columns of tracks and echonest_metrics\n",
    "echo_tracks = pd.merge(\n",
    "    echonest_metrics, tracks[[\"track_id\", \"genre_top\"]], on=\"track_id\"\n",
    ")\n",
    "\n",
    "# Inspect the resultant dataframe\n",
    "echo_tracks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Define our features\\nfeatures = echo_tracks.drop(columns=[\\\"genre_top\\\", \\\"track_id\\\"], axis=1)\\n\\n# Define our labels\\nlabels = echo_tracks[\\\"genre_top\\\"]\";\n",
       "                var nbb_formatted_code = \"# Define our features\\nfeatures = echo_tracks.drop(columns=[\\\"genre_top\\\", \\\"track_id\\\"], axis=1)\\n\\n# Define our labels\\nlabels = echo_tracks[\\\"genre_top\\\"]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define our features\n",
    "features = echo_tracks.drop(columns=[\"genre_top\", \"track_id\"], axis=1)\n",
    "\n",
    "# Define our labels\n",
    "labels = echo_tracks[\"genre_top\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"from sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    features, labels, test_size=0.3, random_state=0\\n)\";\n",
       "                var nbb_formatted_code = \"from sklearn.model_selection import train_test_split\\n\\nX_train, X_test, y_train, y_test = train_test_split(\\n    features, labels, test_size=0.3, random_state=0\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: \n",
      " 0.9222761970853574\n",
      "Random Forest: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.84      0.73      0.79       279\n",
      "        Rock       0.94      0.97      0.95      1162\n",
      "\n",
      "    accuracy                           0.92      1441\n",
      "   macro avg       0.89      0.85      0.87      1441\n",
      "weighted avg       0.92      0.92      0.92      1441\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 205,   74],\n",
       "       [  38, 1124]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"# MODELO COM PIPELINE\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.impute import SimpleImputer\\n\\n# Criando o Preprocessamento no Pipeline\\nnumeric_transformer = Pipeline(\\n    steps=[(\\\"imputer\\\", SimpleImputer(strategy=\\\"median\\\")), (\\\"scaler\\\", StandardScaler())]\\n)\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[(\\\"num\\\", numeric_transformer, X_train.columns)]\\n)\\n\\n# Pipeline com random forest\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nrf = Pipeline(\\n    steps=[(\\\"preprocessor\\\", preprocessor), (\\\"classifier\\\", RandomForestClassifier())]\\n)\\n\\n\\n# Fita o modelo\\nrf.fit(X_train, y_train)\\n\\n# predicoes\\ny_pred = rf.predict(X_test)\\n\\n# Acur\\u00e1cia do modelo\\nfrom sklearn.metrics import accuracy_score\\n\\nprint(\\\"Random Forest Accuracy: \\\\n\\\", accuracy_score(y_test, y_pred))\\n\\n# Classification report\\nfrom sklearn.metrics import classification_report\\n\\nclass_rep_rf = classification_report(y_test, y_pred)\\n\\nprint(\\\"Random Forest: \\\\n\\\", class_rep_rf)\\n\\n# Matriz de Confusao\\nfrom sklearn.metrics import confusion_matrix\\n\\nconfusion_matrix(y_test, y_pred)\";\n",
       "                var nbb_formatted_code = \"# MODELO COM PIPELINE\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.impute import SimpleImputer\\n\\n# Criando o Preprocessamento no Pipeline\\nnumeric_transformer = Pipeline(\\n    steps=[(\\\"imputer\\\", SimpleImputer(strategy=\\\"median\\\")), (\\\"scaler\\\", StandardScaler())]\\n)\\n\\npreprocessor = ColumnTransformer(\\n    transformers=[(\\\"num\\\", numeric_transformer, X_train.columns)]\\n)\\n\\n# Pipeline com random forest\\nfrom sklearn.ensemble import RandomForestClassifier\\n\\nrf = Pipeline(\\n    steps=[(\\\"preprocessor\\\", preprocessor), (\\\"classifier\\\", RandomForestClassifier())]\\n)\\n\\n\\n# Fita o modelo\\nrf.fit(X_train, y_train)\\n\\n# predicoes\\ny_pred = rf.predict(X_test)\\n\\n# Acur\\u00e1cia do modelo\\nfrom sklearn.metrics import accuracy_score\\n\\nprint(\\\"Random Forest Accuracy: \\\\n\\\", accuracy_score(y_test, y_pred))\\n\\n# Classification report\\nfrom sklearn.metrics import classification_report\\n\\nclass_rep_rf = classification_report(y_test, y_pred)\\n\\nprint(\\\"Random Forest: \\\\n\\\", class_rep_rf)\\n\\n# Matriz de Confusao\\nfrom sklearn.metrics import confusion_matrix\\n\\nconfusion_matrix(y_test, y_pred)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODELO COM PIPELINE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Criando o Preprocessamento no Pipeline\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", numeric_transformer, X_train.columns)]\n",
    ")\n",
    "\n",
    "# Pipeline com random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "\n",
    "# Fita o modelo\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predicoes\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Acurácia do modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Random Forest Accuracy: \\n\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class_rep_rf = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest: \\n\", class_rep_rf)\n",
    "\n",
    "# Matriz de Confusao\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andremota/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:  0.9226428924685601\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# Avalia\\u00e7\\u00e3o com kfold\\nfrom sklearn.model_selection import KFold, cross_val_score\\nimport numpy as np\\n\\n# Set up our K-fold cross-validation\\nkf = KFold(n_splits=10, random_state=10)\\n\\nrf = RandomForestClassifier(random_state=10)\\n\\n# Train our models using KFold cv\\nrf_score = cross_val_score(rf, X_train, y_train, cv=kf)\\n\\n# Print the mean of each array of scores\\nprint(\\\"Random Forest: \\\", np.mean(rf_score))\";\n",
       "                var nbb_formatted_code = \"# Avalia\\u00e7\\u00e3o com kfold\\nfrom sklearn.model_selection import KFold, cross_val_score\\nimport numpy as np\\n\\n# Set up our K-fold cross-validation\\nkf = KFold(n_splits=10, random_state=10)\\n\\nrf = RandomForestClassifier(random_state=10)\\n\\n# Train our models using KFold cv\\nrf_score = cross_val_score(rf, X_train, y_train, cv=kf)\\n\\n# Print the mean of each array of scores\\nprint(\\\"Random Forest: \\\", np.mean(rf_score))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Avaliação com kfold\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Set up our K-fold cross-validation\n",
    "kf = KFold(n_splits=10, random_state=10)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=10)\n",
    "\n",
    "# Train our models using KFold cv\n",
    "rf_score = cross_val_score(rf, X_train, y_train, cv=kf)\n",
    "\n",
    "# Print the mean of each array of scores\n",
    "print(\"Random Forest: \", np.mean(rf_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo foi bem melhor que o que foi feito no projeto. O único problema é que está errando  mais para o hip-hop. <br>\n",
    "Uma alternativa é usar class weight ou alguma técnica de oversampling/undersampling.<br>\n",
    "Veremos o que conseguimos fazer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TENTATIVAS DE TRATAMENTO DO DESBALANCEAMENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 CLASS_WEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ideia do class weight é aplicar pesos diferentes para classes diferentes. Claro que não há uma resposta simples para qual peso aplicar. Normalmente o que eu faço é pegar a diferença entre as classes e testar valores nos entornos dela. Exemplo: imagine que a classe 1 é 4x maior que a classe 0. Então, podemos testar a classe 0 com valor 1 e testar alguns valores para a classe 1 em torno de 4 (tipo testar de 2 a 10, podendo até tentar decimais com o np.linspace). Algo que já vi fazerem foi testar valores i em np.linspace(), sendo que a classe 0 recebe i e a classe 1 recebe 1-i. Outra coisa que rola de fazer também é testar com \"balanced\", costuma dar bom também!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"from sklearn.metrics import precision_recall_fscore_support as score\\n\\n\\ndef trying_class_weight(class_weight):\\n    \\\"\\\"\\\" FUN\\u00c7\\u00c3O PARA TESTAR O RESULTADO DE DIFERENTES PESOS PARA AS CLASSES \\\"\\\"\\\"\\n    # Criando o Preprocessamento no Pipeline\\n    numeric_transformer = Pipeline(\\n        steps=[\\n            (\\\"imputer\\\", SimpleImputer(strategy=\\\"median\\\")),\\n            (\\\"scaler\\\", StandardScaler()),\\n        ]\\n    )\\n\\n    preprocessor = ColumnTransformer(\\n        transformers=[(\\\"num\\\", numeric_transformer, X_train.columns)]\\n    )\\n\\n    # Pipeline com random forest\\n    rf = Pipeline(\\n        steps=[\\n            (\\\"preprocessor\\\", preprocessor),\\n            (\\\"classifier\\\", RandomForestClassifier(class_weight=class_weight)),\\n        ]\\n    )\\n\\n    # Fita o modelo\\n    rf.fit(X_train, y_train)\\n\\n    # predicoes\\n    y_pred = rf.predict(X_test)\\n\\n    # Matriz de Confusao\\n    cm = confusion_matrix(y_test, y_pred)\\n    accuracy = (cm[0, 0] + cm[1, 1]) / len(y_test)\\n\\n    precision, recall, fscore, support = score(y_test, y_pred, average=\\\"macro\\\")\\n\\n    return accuracy, fscore\";\n",
       "                var nbb_formatted_code = \"from sklearn.metrics import precision_recall_fscore_support as score\\n\\n\\ndef trying_class_weight(class_weight):\\n    \\\"\\\"\\\" FUN\\u00c7\\u00c3O PARA TESTAR O RESULTADO DE DIFERENTES PESOS PARA AS CLASSES \\\"\\\"\\\"\\n    # Criando o Preprocessamento no Pipeline\\n    numeric_transformer = Pipeline(\\n        steps=[\\n            (\\\"imputer\\\", SimpleImputer(strategy=\\\"median\\\")),\\n            (\\\"scaler\\\", StandardScaler()),\\n        ]\\n    )\\n\\n    preprocessor = ColumnTransformer(\\n        transformers=[(\\\"num\\\", numeric_transformer, X_train.columns)]\\n    )\\n\\n    # Pipeline com random forest\\n    rf = Pipeline(\\n        steps=[\\n            (\\\"preprocessor\\\", preprocessor),\\n            (\\\"classifier\\\", RandomForestClassifier(class_weight=class_weight)),\\n        ]\\n    )\\n\\n    # Fita o modelo\\n    rf.fit(X_train, y_train)\\n\\n    # predicoes\\n    y_pred = rf.predict(X_test)\\n\\n    # Matriz de Confusao\\n    cm = confusion_matrix(y_test, y_pred)\\n    accuracy = (cm[0, 0] + cm[1, 1]) / len(y_test)\\n\\n    precision, recall, fscore, support = score(y_test, y_pred, average=\\\"macro\\\")\\n\\n    return accuracy, fscore\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "def trying_class_weight(class_weight):\n",
    "    \"\"\" FUNÇÃO PARA TESTAR O RESULTADO DE DIFERENTES PESOS PARA AS CLASSES \"\"\"\n",
    "    # Criando o Preprocessamento no Pipeline\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric_transformer, X_train.columns)]\n",
    "    )\n",
    "\n",
    "    # Pipeline com random forest\n",
    "    rf = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", RandomForestClassifier(class_weight=class_weight)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fita o modelo\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # predicoes\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Matriz de Confusao\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = (cm[0, 0] + cm[1, 1]) / len(y_test)\n",
    "\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    return accuracy, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For balanced:\n",
      "0.9215822345593337\n",
      "0.8656028775624576\n",
      "For None:\n",
      "0.9215822345593337\n",
      "0.8672265180035502\n",
      "For 1:4\n",
      "0.919500346981263\n",
      "0.8659061737902709\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# Primeiro, algumas tentativas padr\\u00e3o:\\naccuracy, fscore = trying_class_weight(\\\"balanced\\\")\\nprint(\\\"For balanced:\\\")\\nprint(accuracy)\\nprint(fscore)\\n\\naccuracy, fscore = trying_class_weight(None)\\nprint(\\\"For None:\\\")\\nprint(accuracy)\\nprint(fscore)\\n\\naccuracy, fscore = trying_class_weight({\\\"Hip-Hop\\\": 1, \\\"Rock\\\": 4})\\nprint(\\\"For 1:4\\\")\\nprint(accuracy)\\nprint(fscore)\";\n",
       "                var nbb_formatted_code = \"# Primeiro, algumas tentativas padr\\u00e3o:\\naccuracy, fscore = trying_class_weight(\\\"balanced\\\")\\nprint(\\\"For balanced:\\\")\\nprint(accuracy)\\nprint(fscore)\\n\\naccuracy, fscore = trying_class_weight(None)\\nprint(\\\"For None:\\\")\\nprint(accuracy)\\nprint(fscore)\\n\\naccuracy, fscore = trying_class_weight({\\\"Hip-Hop\\\": 1, \\\"Rock\\\": 4})\\nprint(\\\"For 1:4\\\")\\nprint(accuracy)\\nprint(fscore)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Primeiro, algumas tentativas padrão:\n",
    "accuracy, fscore = trying_class_weight(\"balanced\")\n",
    "print(\"For balanced:\")\n",
    "print(accuracy)\n",
    "print(fscore)\n",
    "\n",
    "accuracy, fscore = trying_class_weight(None)\n",
    "print(\"For None:\")\n",
    "print(accuracy)\n",
    "print(fscore)\n",
    "\n",
    "accuracy, fscore = trying_class_weight({\"Hip-Hop\": 1, \"Rock\": 4})\n",
    "print(\"For 1:4\")\n",
    "print(accuracy)\n",
    "print(fscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced e None deram quase a mesma coisa, que bizarro. Pior, para 1:4 o modelo cagou mais ainda. Vamos fazer novas tentativas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"# Vamos testando alguns n\\u00fameros entre 2 e 8 para rock (100 tentativas)\\nweights = []\\naccuracies = []\\nf1scores = []\\nfor i in np.linspace(2, 8, 100):\\n    accuracy, fscore = trying_class_weight({\\\"Hip-Hop\\\": 1, \\\"Rock\\\": i})\\n    weights.append(i)\\n    accuracies.append(accuracy)\\n    f1scores.append(fscore)\";\n",
       "                var nbb_formatted_code = \"# Vamos testando alguns n\\u00fameros entre 2 e 8 para rock (100 tentativas)\\nweights = []\\naccuracies = []\\nf1scores = []\\nfor i in np.linspace(2, 8, 100):\\n    accuracy, fscore = trying_class_weight({\\\"Hip-Hop\\\": 1, \\\"Rock\\\": i})\\n    weights.append(i)\\n    accuracies.append(accuracy)\\n    f1scores.append(fscore)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos testando alguns números entre 2 e 8 para rock (100 tentativas)\n",
    "weights = []\n",
    "accuracies = []\n",
    "f1scores = []\n",
    "\n",
    "# trying_class_weight é uma função que aplica o peso e ve acuracia e f1-score\n",
    "for i in np.linspace(2, 8, 100):\n",
    "    accuracy, fscore = trying_class_weight({\"Hip-Hop\": 1, \"Rock\": i})\n",
    "    weights.append(i)\n",
    "    accuracies.append(accuracy)\n",
    "    f1scores.append(fscore) # será que dá para criar um dict aqui?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926439972241499\n",
      "6.606060606060606\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"print(max(accuracies))\\nprint(weights[accuracies.index(max(accuracies))])\";\n",
       "                var nbb_formatted_code = \"print(max(accuracies))\\nprint(weights[accuracies.index(max(accuracies))])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(max(accuracies))\n",
    "print(weights[accuracies.index(max(accuracies))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na melhor situação, que foi com 1:6.06, o ganho foi de menos de 0.5pp, muito pouco... #chateado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andremota/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andremota/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Vamos de tentar isso aqui que achei pela internet e ainda n\\u00e3o entendi bem a l\\u00f3gica matem\\u00e1tica \\nweights = []\\naccuracies = []\\nf1scores = []\\nfor i in np.linspace(0.0, 1, 100):\\n    accuracy, fscore = trying_class_weight({\\\"Hip-Hop\\\": i, \\\"Rock\\\": 1 - i})\\n    weights.append(i)\\n    accuracies.append(accuracy)\\n    f1scores.append(fscore)\";\n",
       "                var nbb_formatted_code = \"# Vamos de tentar isso aqui que achei pela internet e ainda n\\u00e3o entendi bem a l\\u00f3gica matem\\u00e1tica\\nweights = []\\naccuracies = []\\nf1scores = []\\nfor i in np.linspace(0.0, 1, 100):\\n    accuracy, fscore = trying_class_weight({\\\"Hip-Hop\\\": i, \\\"Rock\\\": 1 - i})\\n    weights.append(i)\\n    accuracies.append(accuracy)\\n    f1scores.append(fscore)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vamos de tentar isso aqui que achei pela internet e ainda não entendi bem a lógica matemática \n",
    "weights = []\n",
    "accuracies = []\n",
    "f1scores = []\n",
    "for i in np.linspace(0.0, 1, 100):\n",
    "    accuracy, fscore = trying_class_weight({\"Hip-Hop\": i, \"Rock\": 1 - i})\n",
    "    weights.append(i)\n",
    "    accuracies.append(accuracy)\n",
    "    f1scores.append(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9257460097154754\n",
      "0.3434343434343435\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"print(max(accuracies))\\nprint(weights[accuracies.index(max(accuracies))])\";\n",
       "                var nbb_formatted_code = \"print(max(accuracies))\\nprint(weights[accuracies.index(max(accuracies))])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(max(accuracies))\n",
    "print(weights[accuracies.index(max(accuracies))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ainda tá meio merda. Vamos de SMOTE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. EXTRAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOMIZED SEARCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tunar os hiperparâmetros da Random Forest, mas com o Randomized porque o Grid vai levar a vida toda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_random = RandomizedSearchCV(rf, param_distributions = random_grid, n_iter = 300, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# best model\n",
    "print(rf_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO COM PIPELINE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Criando o Preprocessamento no Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, X_train.columns)])\n",
    "\n",
    "# Pipeline com random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(n_estimators= 200,\n",
    "                                                            min_samples_split=10,\n",
    "                                                            min_samples_leaf= 1,\n",
    "                                                            max_features='sqrt',\n",
    "                                                            max_depth= 40,\n",
    "                                                            bootstrap= False))])\n",
    "\n",
    "\n",
    "# Fita o modelo\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predicoes\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Acurácia do modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Random Forest Accuracy: \\n\",accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "class_rep_rf = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest: \\n\", class_rep_rf)\n",
    "\n",
    "# Matriz de Confusao\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja que o modelo não melhorou, e piorou em alguns casos (f1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINE PARA SELEÇÃO DO MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de tentarmos melhorar o balanceamento, vamos só fazer uma brincadeirinha aqui e testar vários modelos com os parâmetros padrão mesmo e ver qual tem melhor resultado. Mera curiosidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    SVC(),\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curioso, sem brincar com os hiperparâmetros, a Random Forest empatou com Gradient Boosting.\n",
    "\n",
    "E modelos mais robustos com XGBoost e CatBoost? Será que faz muita diferença?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "\n",
    "model_cb = CatBoostClassifier(\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    # learning_rate=0.12655172413793103,\n",
    "    # iterations=550,\n",
    "    # border_count=150,\n",
    ")\n",
    "\n",
    "model_cb.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_cb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix : \\n\" + str(confusion_matrix(y_test, y_pred)))a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sensitivity = cm[1, 1] / (cm[1, 1] + cm[1, 0])\n",
    "specificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "accuracy = (cm[0, 0] + cm[1, 1]) / len(y_test)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP VALUES\n",
    "import shap\n",
    "\n",
    "shap_values = model_cb.get_feature_importance(\n",
    "    Pool(X_train, y_train,), type=\"ShapValues\",\n",
    ")\n",
    "\n",
    "expected_value = shap_values[0, -1]\n",
    "shap_values = shap_values[:, :-1]\n",
    "\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "\n",
    "plt.savefig(\"shap_values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beleza, essa brincadeira não levou a lugar algum. Vamos voltar ao jogo sério e usar Class Weight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que já conseguimos ir de 91.00 para 91.26. Além de melhorar o f1-score.<br>\n",
    "Podemos tentar ainda:\n",
    "- Aprimorar esse class weight;\n",
    "- Tunar os hiperparâmetros;\n",
    "- Tentar um xgboost (que tem levado vários campeonatos por aí).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTRAS MELHORIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunar hiperparametros\n",
    "# Tentar outros thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n",
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
